# ğŸ¥ YouTube Sentiment Analyzer with Kafka Streaming

A real-time YouTube video and live stream sentiment analysis dashboard built using:

* ğŸ§  **TextBlob**, **BERT**, and **Azure Cognitive Services** for multi-model sentiment analysis
* ğŸŸ¢ **Aiven Kafka** for comment streaming and message queuing
* ğŸ“Š **Streamlit** for live dashboarding
* â˜ï¸ **YouTube Data API v3** for fetching comments and live chats
* ğŸ”’ SSL-secured Kafka for production-level deployment

---

## ğŸš€ Features

* ğŸ” Analyze **YouTube video** or **live stream** comments in real-time
* ğŸ§ª Choose between 3 sentiment engines:

  * **TextBlob** â€“ lightweight and quick
  * **BERT (Multilingual)** â€“ transformer-based deep learning model
  * **Azure Text Analytics** â€“ enterprise-level API integration
* ğŸ“Š Live visualizations:

  * Pie chart for sentiment distribution
  * Word cloud of frequently used words
* ğŸ” Kafka integration for streaming comments to topics:

  * `youtube-video-comments`
  * `youtube-live-comments`

---

## ğŸ› ï¸ Setup Instructions

### 1. ğŸ” Prerequisites

* Python 3.8+
* Aiven Kafka account with SSL credentials
* Azure Cognitive Services key and endpoint (Text Analytics)
* YouTube Data API v3 key

### 2. âš™ï¸ Environment Variables

Create a `.env` file in your project root with the following content:

```env
YOUTUBE_API_KEY=your_youtube_api_key
AZURE_KEY=your_azure_text_analytics_key
AZURE_ENDPOINT=https://your-resource-name.cognitiveservices.azure.com/

KAFKA_BOOTSTRAP_SERVERS=your_aiven_kafka_bootstrap_url
AIVEN_CA_PATH=/path/to/ca.pem
AIVEN_CERT_PATH=/path/to/service.cert
AIVEN_KEY_PATH=/path/to/service.key
```

### 3. ğŸ“† Install Dependencies

Install all required packages using pip:

```bash
pip install -r requirements.txt
```

Sample `requirements.txt`:

```txt
streamlit
kafka-python
google-api-python-client
textblob
transformers
torch
azure-ai-textanalytics
azure-core
python-dotenv
wordcloud
matplotlib
```

---

## ğŸ§  How It Works

1. User inputs a YouTube video or live stream URL.
2. The app determines if it's a video or live stream and fetches comments accordingly.
3. Each comment is:

   * Analyzed for sentiment using selected method
   * Sent to Kafka topic for downstream processing
   * Counted and visualized in real-time
4. Dashboard updates sentiment distribution and comment word cloud every few seconds.

![System Block Diagram](images/block_diagram.png)  
*System architecture showing sentiment engine, Kafka flow, and dashboard integration*
---

## ğŸ“¸ Dashboard UI

Hereâ€™s a preview of the interactive Streamlit dashboard:

![Dashboard - TextBlob](images/dashboard_textblob.png)  

![Dashboard - Bert](images/dashboard_bert.png)  

![Dashboard - Bert](images/dashboard_azure.png)  

![Dashboard - TextBlob-livestream video](images/dashboard_textblob_live.png)  

![Dashboard - Bert-livestream video](images/dashboard_bert_live.png)  

![Dashboard - Bert-livestream video](images/dashboard_azure_live.png)  


---

## ğŸ›„ Kafka Topics

* `youtube-video-comments` â€“ for video comments
* `youtube-live-comments` â€“ for live stream chats

---

## ğŸ“Œ Notes

* Azure API may incur costs beyond free tier.
* BERT model is resource-intensive. A GPU is recommended for large-scale usage.
* Kafka SSL files (CA, CERT, KEY) must be stored securely and not committed to version control.

---

## ğŸ§ª To Run

```bash
streamlit run main.py
```

Then open your browser at [http://localhost:8501](http://localhost:8501)

---

## ğŸ“„ License

This project is licensed under the MIT License.

---

## ğŸ™Œ Acknowledgments

* Hugging Face Transformers (BERT)
* Azure Cognitive Services
* Aiven Kafka
* Google YouTube Data API
